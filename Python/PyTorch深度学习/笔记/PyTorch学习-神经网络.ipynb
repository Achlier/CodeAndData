{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc2ca7f",
   "metadata": {},
   "source": [
    "# PyTorch学习-神经网络（LS16-LS36）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe23a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff4d9b0",
   "metadata": {},
   "source": [
    "## LS17.激活函数与LOSS的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc45ce",
   "metadata": {},
   "source": [
    "### 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1664ba77",
   "metadata": {},
   "source": [
    "$$\n",
    "sigmoid(x)=\\frac1{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "sigmoid'(x)=sigmoid(x)(1-sigmoid(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "15cf409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5398e-05, 4.1877e-04, 3.8510e-03, 3.4445e-02, 2.4766e-01, 7.5234e-01,\n",
      "        9.6555e-01, 9.9615e-01, 9.9958e-01, 9.9995e-01])\n"
     ]
    }
   ],
   "source": [
    "a=torch.linspace(-10,10,10)\n",
    "print(torch.sigmoid(a)) # 小心梯度弥散问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d2a9ff",
   "metadata": {},
   "source": [
    "$$\n",
    "tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}=2sigmoid(2x)-1\n",
    "$$\n",
    "\n",
    "$$\n",
    "tanh'(x)=1-tanh^2(x)\n",
    "$$\n",
    "\n",
    "X轴压缩2倍，Y轴放大两倍，再向下平移1。范围 \\[-1,1\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "705fb175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0000, -1.0000, -1.0000, -0.9975, -0.8045,  0.8045,  0.9975,  1.0000,\n",
      "         1.0000,  1.0000])\n"
     ]
    }
   ],
   "source": [
    "a=torch.linspace(-10,10,10)\n",
    "print(torch.tanh(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7371893c",
   "metadata": {},
   "source": [
    "#### Rectified Linear Unit\n",
    "\n",
    "$$\n",
    "ReLU=\\begin{cases} 0 & \\text{for } x<0 \\\\ x & \\text{for } x\\ge0 \\end{cases}\n",
    "$$\n",
    "\n",
    "- 如果将小于零的线段增加一定幅度，则是 Leaky ReLU\n",
    "- 如果将小于零的线段改成 sigmoid，则是 SELU\n",
    "- 如果对拐点进行平滑处理，则是 Softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8a48d7e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.1111,  3.3333,  5.5556,\n",
      "         7.7778, 10.0000])\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.1111,  3.3333,  5.5556,\n",
      "         7.7778, 10.0000])\n"
     ]
    }
   ],
   "source": [
    "a=torch.linspace(-10,10,10)\n",
    "print(torch.relu(a))\n",
    "print(F.relu(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d939b1",
   "metadata": {},
   "source": [
    "### Loss函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c13c78e",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE=\\sum(f_\\theta(x)-y)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla MSE=2\\sum(f_\\theta(x)-y)*\\frac{\\nabla f_\\theta(x)}{\\nabla \\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cef0170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<MseLossBackward0>)\n",
      "(tensor([2.]),)\n"
     ]
    }
   ],
   "source": [
    "#用autograd.grad自动求导,torch.autograd.grad(loss,[w1,w2,...]),返回[w1 grad,w2 grad,...]\n",
    "\n",
    "x=torch.ones(1)\n",
    "w=torch.full([1],2).float() # y=wx,x=1,w=2\n",
    "w.requires_grad_()  #允许计算梯度，没有这个会报错\n",
    "mse=F.mse_loss(x*w,torch.ones(1))  # 更新动态图, y=1\n",
    "print(mse) # (2-1)^2\n",
    "print(torch.autograd.grad(mse,[w]))  # 输出值（mse）对输入变量（w）求导, 2*(2-1)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e7182390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "#用backward函数，会自动计算所有的梯度大小，可以调用w1.grad和 w2.grad\n",
    "\n",
    "mse=F.mse_loss(x*w,torch.ones(1))\n",
    "mse.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9155c10f",
   "metadata": {},
   "source": [
    "#### Soft version of max\n",
    "\n",
    "$$\n",
    "y=\\begin{matrix} 2.0 \\\\ 1.0 \\\\ 0.1 \\end{matrix}\\Rightarrow p_i=Softmax(y_i)=\\frac{e^{y_i}}{\\sum_ke^{y_k}}=\\begin{matrix} 0.7 \\\\ 0.2 \\\\ 0.1 \\end{matrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla Softmax(y_i)=\\frac{\\partial p_i}{\\partial a_j}=\\begin{cases} p_i(1-p_i) & \\text{if } i=j \\\\ -p_j·p_i & \\text{if } i\\ne j \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c4614b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.1005,  0.2052, -0.1048]),)\n",
      "(tensor([-0.1265, -0.1048,  0.2313]),)\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(3)\n",
    "a.requires_grad_()\n",
    "p=F.softmax(a,dim=0)\n",
    "print(torch.autograd.grad(p[1],[a],retain_graph=True)) # Loss只能1维\n",
    "print(torch.autograd.grad(p[2],[a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf101bea",
   "metadata": {},
   "source": [
    "## LS19.感知机的梯度推导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252dd2e8",
   "metadata": {},
   "source": [
    "### 单一感知机\n",
    "\n",
    "<img src=\"pic\\pic1.jpg\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a63bd8",
   "metadata": {},
   "source": [
    "其中损失函数表示为：$E=\\frac12(O_0^1-t)^2$\n",
    "\n",
    "$sigmoid$ 函数求导：$g'(z)=g(z)(1-g(z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e12558",
   "metadata": {},
   "source": [
    "#### 梯度求导过程\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\\frac{\\partial E}{\\partial w_{j0}}&=\\left(O_{0}-t\\right) \\frac{\\partial O_{0}}{\\partial w_{j0}} \\\\\n",
    "\\frac{\\partial E}{\\partial w_{j0}}&=\\left(O_{0}-t\\right) \\frac{\\partial \\sigma\\left(x_{0}\\right)}{\\partial w_{j0}} \\\\\n",
    "\\frac{\\partial E}{\\partial w_{j0}}&=\\left(O_{0}-t\\right) \\sigma\\left(x_{0}\\right)\\left(1-\\sigma\\left(x_{0}\\right)\\right) \\frac{\\partial x_{0}^{1}}{\\partial w_{j0}} \\\\\n",
    "\\frac{\\partial E}{\\partial w_{j0}}&=\\left(O_{0}-t\\right) O_{0}\\left(1-O_{0}\\right) \\frac{\\partial x_{0}^{1}}{\\partial w_{j0}} \\\\\n",
    "\\frac{\\partial E}{\\partial w_{j0}}&=\\left(O_{0}-t\\right) O_{0}\\left(1-O_{0}\\right) x_{j}^{0}\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce1a78b",
   "metadata": {},
   "source": [
    "#### 代码运用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c46cc5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0891, -0.0548, -0.0099, -0.1739,  0.0405, -0.1314, -0.0911, -0.0527,\n",
      "         -0.0367, -0.0011]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(1,10)\n",
    "w=torch.randn(1,10,requires_grad=True)\n",
    "o=torch.sigmoid(x@w.t()) #矩阵式乘法\n",
    "\n",
    "loss=F.mse_loss(torch.ones(1,1),o)\n",
    "\n",
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42981c45",
   "metadata": {},
   "source": [
    "### 多输出感知机\n",
    "\n",
    "<img src=\"pic\\pic2.jpg\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d67d6",
   "metadata": {},
   "source": [
    "其中损失函数表示为：$E=\\frac12(O_i^1-t_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a993770c",
   "metadata": {},
   "source": [
    "#### 梯度求导过程\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E}{\\partial w_{j k}}&=\\left(O_{k}-t_{k}\\right) \\frac{\\partial O_{k}}{\\partial w_{j k}} \\\\\n",
    "\\frac{\\partial E}{\\partial w_{j k}}&=\\left(O_{\\mathrm{k}}-t_{k}\\right) \\frac{\\partial \\sigma\\left(x_{k}\\right)}{\\partial w_{j k}} \\\\\n",
    "\\frac{\\partial E}{\\partial w_{j k}}&=\\left(O_{k}-t_{k}\\right) \\sigma\\left(x_{k}\\right)\\left(1-\\sigma\\left(x_{k}\\right)\\right) \\frac{\\partial x_{k}^{1}}{\\partial w_{j k}} \\\\\n",
    "\\frac{\\partial E}{\\partial w_{j k}}&=\\left(O_{k}-t_{k}\\right) O_{\\mathrm{k}}\\left(1-O_{k}\\right) \\frac{\\partial x_{k}^{1}}{\\partial w_{j k}} \\\\\n",
    "\\frac{\\partial E}{\\partial w_{j k}}&=\\left(O_{k}-t_{k}\\right) O_{\\mathrm{k}}\\left(1-O_{k}\\right) x_{j}^{0}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40ca9b0",
   "metadata": {},
   "source": [
    "#### 代码运用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7830943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0043, -0.0066, -0.0252, -0.0253, -0.0119,  0.0927, -0.1116,  0.0628,\n",
      "          0.0308, -0.0127],\n",
      "        [-0.0026, -0.0040, -0.0151, -0.0152, -0.0071,  0.0556, -0.0669,  0.0377,\n",
      "          0.0185, -0.0076]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(1,10)\n",
    "w=torch.randn(2,10,requires_grad=True)\n",
    "o=torch.sigmoid(x@w.t()) # Broadcasting 自动拓展\n",
    "\n",
    "loss=F.mse_loss(torch.ones(1,2),o)\n",
    "\n",
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29489795",
   "metadata": {},
   "source": [
    "## LS20.引入链式法则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd75ede",
   "metadata": {},
   "source": [
    "<img src=\"pic\\pic3.jpg\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b835acb",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{j k}^{1}}=\\frac{\\partial E}{\\partial O_{k}^{1}} \\frac{\\partial O_{k}^{1}}{\\partial x}=\\frac{\\partial E}{\\partial O_{k}^{2}} \\frac{\\partial O_{k}^{2}}{\\partial O_{k}^{1}} \\frac{\\partial O_{k}^{1}}{\\partial x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fdb8dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(1.)\n",
    "w1=torch.tensor(2.,requires_grad=True)\n",
    "b1=torch.tensor(1.)\n",
    "w2=torch.tensor(2.,requires_grad=True)\n",
    "b2=torch.tensor(1.)\n",
    "\n",
    "y1=x*w1+b1\n",
    "y2=y1*w2+b2\n",
    "\n",
    "dy2_dy1=torch.autograd.grad(y2,[y1],retain_graph=True)[0]\n",
    "dy1_dw1=torch.autograd.grad(y1,[w1],retain_graph=True)[0]\n",
    "dy2_dw1=torch.autograd.grad(y2,[w1],retain_graph=True)[0]\n",
    "\n",
    "print(dy2_dy1*dy1_dw1)\n",
    "print(dy2_dw1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f29e1",
   "metadata": {},
   "source": [
    "## LS21.多层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f6a1ae",
   "metadata": {},
   "source": [
    "<img src=\"pic\\pic4.jpg\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122629f",
   "metadata": {},
   "source": [
    "为了方便运算，设置\n",
    "\n",
    "$$\n",
    "\\delta_k^K=\\left(O_{k}-t_{k}\\right) O_{\\mathrm{k}}\\left(1-O_{k}\\right) \n",
    "$$\n",
    "\n",
    "以上数据都是可以直接求出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21040ef",
   "metadata": {},
   "source": [
    "#### 梯度求导过程\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E}{\\partial W_{i j}} &=\\frac{\\partial}{\\partial W_{i j}} \\frac{1}{2} \\sum_{k \\in K}\\left(O_{k}-t_{k}\\right)^{2} \\\\\n",
    "\\frac{\\partial E}{\\partial W_{i j}} &=\\sum_{k \\in K}\\left(O_{k}-t_{k}\\right) \\frac{\\partial}{\\partial W_{i j}} O_{k} \\\\\n",
    "\\frac{\\partial E}{\\partial W_{i j}} &=\\sum_{k \\in K}\\left(O_{k}-t_{k}\\right) \\frac{\\partial}{\\partial W_{i j}} \\sigma\\left(x_{k}\\right)\\\\\n",
    "\\frac{\\partial E}{\\partial W_{i j}}&=\\sum_{k \\in K}\\left(O_{k}-t_{k}\\right) \\sigma\\left(x_{k}\\right)\\left(1-\\sigma\\left(x_{k}\\right)\\right) \\frac{\\partial x_{k}}{\\partial W_{i j}} \\\\\n",
    "\\frac{\\partial E}{\\partial W_{i j}}&=\\sum_{k \\in K}\\left(O_{k}-t_{k}\\right) O_{k}\\left(1-O_{k}\\right) \\frac{\\partial x_{k}}{\\partial O_{j}} \\cdot \\frac{\\partial O_{j}}{\\partial W_{i j}} \\\\\n",
    "\\frac{\\partial E}{\\partial W_{i j}}&=\\sum_{k \\in K}\\left(O_{k}-t_{k}\\right) O_{k}\\left(1-O_{k}\\right) W_{j k} \\frac{\\partial O_{j}}{\\partial W_{i j}}\\\\\n",
    "\\frac{\\partial E}{\\partial W_{i j}}&=O_{j}\\left(1-O_{j}\\right) \\frac{\\partial x_{j}}{\\partial W_{i j}} \\sum_{k \\in K}\\left(O_{k}-t_{k}\\right) O_{k}\\left(1-O_{k}\\right) W_{j k}\\\\\n",
    "\\frac{\\partial E}{\\partial W_{i j}}&=O_{i} O_{j}\\left(1-O_{j}\\right) \\sum_{k \\in K} \\delta_{k} W_{j k}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403848e",
   "metadata": {},
   "source": [
    "同样，将\n",
    "$$\n",
    "\\delta_j^J=O_{j}\\left(1-O_{j}\\right) \\sum_{k \\in K} \\delta_{k} W_{j k}\n",
    "$$\n",
    "设置为从J层到以后的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9400ab4",
   "metadata": {},
   "source": [
    "### 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f32ab",
   "metadata": {},
   "source": [
    "For an output layer node $k \\in K$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W_{j k}}=O_{j} \\delta_{k}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\delta_{k}=O_{k}\\left(1-O_{k}\\right)\\left(O_{k}-t_{k}\\right)\n",
    "$$\n",
    "\n",
    "For a hidden layer node $j \\in J$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W_{i j}}=O_{i} \\delta_{j}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\delta_{j}=O_{j}\\left(1-O_{j}\\right) \\sum_{k \\in K} \\delta_{k} W_{j k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8122f2",
   "metadata": {},
   "source": [
    "## LS22.交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666a8e1",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "$$\n",
    "Entropy=-\\sum_iP(i)logP(i)\n",
    "$$\n",
    "\n",
    "- Uncertainty\n",
    "- measure of surprise\n",
    "- higher entropy: higher uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d98bfe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.3568)\n",
      "tensor(0.0342)\n"
     ]
    }
   ],
   "source": [
    "a=torch.full([4],1/4.)\n",
    "print(-(a*torch.log2(a)).sum()) #求  entropy\n",
    "\n",
    "a=torch.tensor([0.1,0.1,0.1,0.7])\n",
    "print(-(a*torch.log2(a)).sum())\n",
    "\n",
    "a=torch.tensor([0.001,0.001,0.001,0.997])\n",
    "print(-(a*torch.log2(a)).sum())\n",
    "\n",
    "#概率值越集中在某一个点，entropy越小；概率值越平均，entropy越大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc2eab6",
   "metadata": {},
   "source": [
    "### Cross Entropy\n",
    "\n",
    "$$\n",
    "H(p,q)=-\\sum p\\log q=H(p)+D_{KL}(p|q),\\quad D_{KL}=\\text{KL Divergence}\n",
    "$$\n",
    "\n",
    "越是相似，KL Divergence越接近0，因此当\n",
    "- P=Q 时，D_{KL}(p|q)=0, Cross Entropy=Entropy\n",
    "- 同时运用 one-hot encoding 时, entropy = 1log1=0\n",
    "- 因此分类问题的 Cross Entropy，相当于对 KL Divergence 的运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca69a6",
   "metadata": {},
   "source": [
    "### Binary Classification\n",
    "\n",
    "$$\n",
    "H(P, Q)=-P(c a t) \\log Q(c a t)-(1-P(c a t)) \\log (1-Q(c a t)) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc93f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.3820)\n",
      "tensor(16.3820)\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(1,784)\n",
    "w=torch.randn(10,784)\n",
    "logits=x@w.t()\n",
    "print(F.cross_entropy(logits,torch.tensor([3])))# cross_entropy=softmax+log+nll_loss\n",
    "\n",
    "pred=F.softmax(logits,dim=1)\n",
    "pred_log=torch.log(pred)\n",
    "print(F.nll_loss(pred_log,torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ade582",
   "metadata": {},
   "source": [
    "## LS30.Visdom可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a6bf91",
   "metadata": {},
   "source": [
    "1. 首先通过 `python -m visdom.server` 建立连接，打开网页\n",
    "2. 然后 `from visdom import Visdom`,`viz = Visdom()`, 加载监听器\n",
    "3. `viz.line([0.], [0.], win='窗口ID', opts=dict(title='窗口标题'))`, [0.][0.]是初始点\n",
    "4. 通过`viz.line([y轴数据],[x轴数据], win='窗口ID', update='append')` 更新数据\n",
    "\n",
    "5. `viz.images()` 展示图片, `viz.text()` 展示字"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9bcbaa",
   "metadata": {},
   "source": [
    "## LS31.防止过拟合的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f3579",
   "metadata": {},
   "source": [
    "- 采用交叉验证\n",
    "- 加入正则项\n",
    "- 采用动量\n",
    "- 采用学习率衰减\n",
    "- Early Stop\n",
    "- Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47e92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
