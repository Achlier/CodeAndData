{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5829b73e",
   "metadata": {},
   "source": [
    "## Logistic Regression 多分类实战（MNIST）\n",
    "\n",
    "此处 Loss function 采用 crossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe7fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch\n",
    "import  torch.nn as nn\n",
    "import  torch.nn.functional as F\n",
    "import  torch.optim as optim\n",
    "from    torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3350ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visdom import Visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b77f45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=200 #Batch Size：一次训练所选取的样本数\n",
    "learning_rate=0.01\n",
    "epochs=3 #1个epoch表示过1遍训练集中的所有样本，这里可以设置为 5\n",
    "\n",
    "#加载数据\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=False, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f74975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1054, -0.0533, -0.0245,  ..., -0.1081,  0.0521,  0.0839],\n",
       "        [ 0.0523, -0.0468,  0.0551,  ..., -0.0937, -0.0934,  0.0383],\n",
       "        [-0.0553, -0.0256,  0.0705,  ..., -0.0381,  0.1084, -0.0544],\n",
       "        ...,\n",
       "        [ 0.1206, -0.0403, -0.0034,  ..., -0.0679,  0.0092, -0.0966],\n",
       "        [ 0.1148,  0.0669, -0.0461,  ..., -0.1810, -0.1067,  0.0905],\n",
       "        [ 0.1352, -0.0624,  0.0797,  ..., -0.0476,  0.1126, -0.0556]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在pytorch中的定义（a，b）a是out输出，b是in输入，也就是（输出，输入）\n",
    "# 比如第一个可以理解为从784降维成200的层\n",
    "w1, b1 = torch.randn(200, 784, requires_grad=True),\\\n",
    "         torch.zeros(200, requires_grad=True)\n",
    "w2, b2 = torch.randn(200, 200, requires_grad=True),\\\n",
    "         torch.zeros(200, requires_grad=True)\n",
    "w3, b3 = torch.randn(10, 200, requires_grad=True),\\\n",
    "         torch.zeros(10, requires_grad=True)\n",
    "\n",
    "#凯明初始化\n",
    "# torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "torch.nn.init.kaiming_normal_(w1)\n",
    "torch.nn.init.kaiming_normal_(w2)\n",
    "torch.nn.init.kaiming_normal_(w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f385c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#前向传播过程\n",
    "def forward(x):\n",
    "    x = x@w1.t() + b1\n",
    "    x = F.relu(x)\n",
    "    x = x@w2.t() + b2\n",
    "    x = F.relu(x)\n",
    "    x = x@w3.t() + b3\n",
    "    x = F.relu(x)  #这里不要用softmax，因为之后的crossEntropyLoss中自带了。这里可以用relu，也可以不用。\n",
    "    return x  #返回的是一个logits（即没有经过sigmoid或者softmax的层）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b640de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#优化器\n",
    "optimizer = optim.SGD([w1, b1, w2, b2, w3, b3], lr=learning_rate)\n",
    "criteon = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1fed2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.622670\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 1.169736\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.597393\n",
      "\n",
      "Test set: Average loss: 0.0030, Accuracy: 8096/10000 (81%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.600992\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.528212\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.460533\n",
      "\n",
      "Test set: Average loss: 0.0025, Accuracy: 8305/10000 (83%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.506943\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.498760\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.455540\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 8401/10000 (84%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.466507\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.545814\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.493273\n",
      "\n",
      "Test set: Average loss: 0.0022, Accuracy: 8464/10000 (85%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.391793\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.471149\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.385770\n",
      "\n",
      "Test set: Average loss: 0.0021, Accuracy: 8497/10000 (85%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.474917\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.469056\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.411693\n",
      "\n",
      "Test set: Average loss: 0.0021, Accuracy: 8517/10000 (85%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.387261\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.454146\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.533029\n",
      "\n",
      "Test set: Average loss: 0.0020, Accuracy: 8551/10000 (86%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.442289\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.440906\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.343846\n",
      "\n",
      "Test set: Average loss: 0.0020, Accuracy: 8567/10000 (86%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.354626\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.368647\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.367205\n",
      "\n",
      "Test set: Average loss: 0.0019, Accuracy: 8584/10000 (86%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.314438\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.443083\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.371531\n",
      "\n",
      "Test set: Average loss: 0.0019, Accuracy: 8609/10000 (86%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(-1, 28*28) #将二维的图片数据打平 [200,784],第5课用的 x = x.view(x.size(0), 28*28)\n",
    "\n",
    "        logits = forward(data) #这里是网络的输出\n",
    "        loss = criteon(logits, target)  #调用cross—entorpy计算输出值和真实值之间的loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "        # len(data)---指的是一个batch_size;\n",
    "        # len(train_loader.dataset)----指的是train_loader这个数据集中总共有多少张图片（数据）\n",
    "        # len(train_loader)---- len(train_loader.dataset)/len(data)---就是这个train_loader要加载多少次batch\n",
    "\n",
    "    # 测试网络---test----每训练完一个epoch检测一下测试结果\n",
    "    # 因为每一个epoch已经优化了batch次参数，得到的参数信息还是OK的\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        logits = forward(data) #logits的shape=[200,10]，--200是batchsize，10是最后输出结果的10分类\n",
    "        test_loss += criteon(logits, target).item()  #每次将test_loss进行累加   #target=[200,1]---每个类只有一个正确结果\n",
    "\n",
    "        pred = logits.data.max(1)[1]\n",
    "        # 这里losgits.data是一个二维数组；其dim=1;max()---返回的是每行的最大值和最大值对应的索引\n",
    "        # max(1)----是指每行取最大值;max(1)[1]---取每行最大值对应的索引号\n",
    "        # 也可以写成 pred=logits.argmax(dim=1)\n",
    "        correct += pred.eq(target.data).sum()\n",
    "        #预测值和目标值相等个数进行求和--在for中，将这个test_loader中相等的个数都求出来\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ab25b",
   "metadata": {},
   "source": [
    "### 模型改进：交叉验证+GPU加速+可视化+正则化+动量+学习率衰减"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f30d2",
   "metadata": {},
   "source": [
    "设置交叉验证，即 训练集：验证集=5:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4accff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=200\n",
    "learning_rate=0.01\n",
    "epochs=3\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/', train=False, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True)# 训练集方法不变\n",
    "\n",
    "train_db = datasets.MNIST('data/', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "train_db, val_db = torch.utils.data.random_split(train_db, [50000, 10000])# 训练集划分两份\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_db,\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_db,\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21fca7",
   "metadata": {},
   "source": [
    "这一部分我们尝试用另一种方法构建全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb7eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 200),\n",
    "            # nn.Dropout(0.5), dropout 50%\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(200, 10),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        #一般采用以下的写法，nn.Linear自带了自己的初始化方式，一般够用\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d145f",
   "metadata": {},
   "source": [
    "并将设备与数据都搬入GPU,同时设置正则化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b7b8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "net = MLP().to(device)\n",
    "criteon = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.01)\n",
    "# 这里设置 momentum=0.9,新增了动量。有些优化器没有 momentum参数，因为优化器本身带有动量影响。\n",
    "# weight_decay=0.01，意思是L2-regularization，lamda设置为0.01\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                         factor=0.1, patience=10, verbose=False, \n",
    "                                         threshold=0.0001, threshold_mode='rel', \n",
    "                                         cooldown=0, min_lr=0, eps=1e-08)\n",
    "#训练过程中，optimizer会把 learning rate 交给scheduler管理\n",
    "#当指标（比如loss）连续patience次数还没有改进时，需要降低学习率，factor为每次下降的比例\n",
    "\n",
    "#或者可以采用 scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=30,gamma=0.1) 每30步将 lr 乘 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b8eef",
   "metadata": {},
   "source": [
    "可以加入 Visdom 可视化功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5189f763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "viz = Visdom()\n",
    "\n",
    "viz.line([0.], [0.], win='train_loss', opts=dict(title='train loss'))\n",
    "viz.line([[0.0, 0.0]], [0.], win='test_loss', opts=dict(title='test loss&acc.',legend=['loss', 'acc.']))\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b41a297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.299585\n",
      "Train Epoch: 0 [20000/50000 (40%)]\tLoss: 2.050581\n",
      "Train Epoch: 0 [40000/50000 (80%)]\tLoss: 1.401711\n",
      "\n",
      "Test set: Average loss: 0.0051, Accuracy: 7266.0/10000 (73%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1.020472\n",
      "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 0.674887\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 0.568124\n",
      "\n",
      "Test set: Average loss: 0.0024, Accuracy: 8711.0/10000 (87%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.479894\n",
      "Train Epoch: 2 [20000/50000 (40%)]\tLoss: 0.492326\n",
      "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 0.381439\n",
      "\n",
      "Test set: Average loss: 0.0019, Accuracy: 8906.0/10000 (89%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    #用train_loader来训练\n",
    "    # net_dropped.train() 启用dropout\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(-1, 28*28)\n",
    "        data, target = data.to(device), target.cuda()\n",
    "\n",
    "        logits = net(data)\n",
    "        loss = criteon(logits, target)\n",
    "        schedular.step(loss) # 检查 loss 的减少有没有长时间静止\n",
    "        \n",
    "        #pytorch里没有自动的L1-regularization\n",
    "        #如果自己写代码将如下：\n",
    "        \"\"\"\n",
    "        regularization_loss=0\n",
    "        for param in model.parameters():\n",
    "        regularization_loss += torch.sum(torch.abs(param))\n",
    "        \n",
    "        classify_loss=criteon(logits, target)\n",
    "        loss=classify_loss+0.01*regularization_loss\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        global_step += 1\n",
    "        viz.line([loss.item()], [global_step], win='train_loss', update='append')\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    #用 val_loader来测试，挑选最好的参数\n",
    "    # net_dropped.eval() 不启用dropout\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in val_loader:\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        data, target = data.to(device), target.cuda()\n",
    "        logits = net(data)\n",
    "        test_loss += criteon(logits, target).item()\n",
    "\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += pred.eq(target).float().sum().item()\n",
    "        \n",
    "    viz.line([[test_loss, correct / len(test_loader.dataset)]],\n",
    "             [global_step], win='test_loss', update='append')\n",
    "    viz.images(data.view(-1, 1, 28, 28)*0.3081+0.1307, win='x')\n",
    "    viz.text(str(pred.detach().cpu().numpy()), win='pred',\n",
    "             opts=dict(title='pred'))\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a97b1d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0018, Accuracy: 8971/10000 (90%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WinError 10054] 远程主机强迫关闭了一个现有的连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n",
      "[WinError 10061] 由于目标计算机积极拒绝，无法连接。\n"
     ]
    }
   ],
   "source": [
    "#多次调参重复上面循环，最后用test_loader来验收结果，这个跑一次后就不能动了。\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    data, target = data.to(device), target.cuda()\n",
    "    logits = net(data)\n",
    "    test_loss += criteon(logits, target).item()\n",
    "\n",
    "    pred = logits.data.max(1)[1]\n",
    "    correct += pred.eq(target.data).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c8501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
